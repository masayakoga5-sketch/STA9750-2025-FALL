[
  {
    "objectID": "docs/mp02/mp02.html",
    "href": "docs/mp02/mp02.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "We are currently working to analyze housing affordability in the United States and use different metrics to see the trends that occured regaridng housing affordiability in this past 2 decades.\nTask 1\n\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n  dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nlibrary &lt;- function(pkg){\n  ## Mask base::library() to automatically install packages if needed\n  ## Masking is important here so downlit picks up packages and links\n  ## to documentation\n  pkg &lt;- as.character(substitute(pkg))\n  options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n  if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n  stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(glue)\nlibrary(readxl)\nlibrary(tidycensus)\n\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n  fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n  fname &lt;- file.path(\"data\", \"mp02\", fname)\n  \n  if(!file.exists(fname)){\n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n    \n    ALL_DATA &lt;- map(YEARS, function(yy){\n      tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n        mutate(year=yy) |&gt;\n        select(-moe, -variable) |&gt;\n        rename(!!variable := estimate)\n    }) |&gt; bind_rows()\n    \n    write_csv(ALL_DATA, fname)\n  }\n  \n  read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n  rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n  rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n  rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n  rename(households = B11001_001)\n\n# Number of new housing units built each year#\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n  fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n  fname &lt;- file.path(\"data\", \"mp02\", fname)\n  \n  if(!file.exists(fname)){\n    HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n    \n    HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n      historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n      \n      LINES &lt;- readLines(historical_url)[-c(1:11)]\n      \n      CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n      CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n      \n      PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n      PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n      \n      data_frame(CBSA = CBSA,\n                 new_housing_units_permitted = PERMITS, \n                 year = yy)\n    }) |&gt; bind_rows()\n    \n    CURRENT_YEARS &lt;- seq(2019, end_year)\n    \n    CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n      current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n      \n      temp &lt;- tempfile()\n      \n      download.file(current_url, destfile = temp, mode=\"wb\")\n      \n      fallback &lt;- function(.f1, .f2){\n        function(...){\n          tryCatch(.f1(...), \n                   error=function(e) .f2(...))\n        }\n      }\n      \n      reader &lt;- fallback(read_xlsx, read_xls)\n      \n      reader(temp, skip=5) |&gt;\n        na.omit() |&gt;\n        select(CBSA, Total) |&gt;\n        mutate(year = yy) |&gt;\n        rename(new_housing_units_permitted = Total)\n    }) |&gt; bind_rows()\n    \n    ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n    \n    write_csv(ALL_DATA, fname)\n    \n  }\n  \n  read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\n#Core-Based Statistical Areas Data Acquisition#\nlibrary(httr2)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nget_bls_industry_codes &lt;- function(){\n  fname &lt;- fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n  \n  if(!file.exists(fname)){\n    \n    resp &lt;- request(\"https://www.bls.gov\") |&gt; \n      req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n      req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n      req_error(is_error = \\(resp) FALSE) |&gt;\n      req_perform()\n    \n    resp_check_status(resp)\n    \n    naics_table &lt;- resp_body_html(resp) |&gt;\n      html_element(\"#naics_titles\") |&gt; \n      html_table() |&gt;\n      mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n      select(-`Industry Title`) |&gt;\n      mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n      filter(!is.na(depth))\n    \n    naics_table &lt;- naics_table |&gt; \n      filter(depth == 4) |&gt; \n      rename(level4_title=title) |&gt; \n      mutate(level1_code = str_sub(Code, end=2), \n             level2_code = str_sub(Code, end=3), \n             level3_code = str_sub(Code, end=4)) |&gt;\n      left_join(naics_table, join_by(level1_code == Code)) |&gt;\n      rename(level1_title=title) |&gt;\n      left_join(naics_table, join_by(level2_code == Code)) |&gt;\n      rename(level2_title=title) |&gt;\n      left_join(naics_table, join_by(level3_code == Code)) |&gt;\n      rename(level3_title=title) |&gt;\n      select(-starts_with(\"depth\")) |&gt;\n      rename(level4_code = Code) |&gt;\n      select(level1_title, level2_title, level3_title, level4_title, \n             level1_code,  level2_code,  level3_code,  level4_code)\n    \n    write_csv(naics_table, fname)\n  }\n  \n  read_csv(fname, show_col_types=FALSE)\n  \n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n#BLS BLS Quarterly Census of Employment and Wages Data Acqusition#\nlibrary(httr2)\nlibrary(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n  fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n  fname &lt;- file.path(\"data\", \"mp02\", fname)\n  \n  YEARS &lt;- seq(start_year, end_year)\n  YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n  \n  if(!file.exists(fname)){\n    ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n      fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n      \n      if(!file.exists(fname_inner)){\n        request(\"https://www.bls.gov\") |&gt; \n          req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                       glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n          req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n          req_retry(max_tries=5) |&gt;\n          req_perform(fname_inner)\n      }\n      \n      if(file.info(fname_inner)$size &lt; 755e5){\n        warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n      }\n      \n      read_csv(fname_inner, \n               show_col_types=FALSE) |&gt; \n        mutate(YEAR = yy) |&gt;\n        select(area_fips, \n               industry_code, \n               annual_avg_emplvl, \n               total_annual_wages, \n               YEAR) |&gt;\n        filter(nchar(industry_code) &lt;= 5, \n               str_starts(area_fips, \"C\")) |&gt;\n        filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n        mutate(FIPS = area_fips, \n               INDUSTRY = as.integer(industry_code), \n               EMPLOYMENT = as.integer(annual_avg_emplvl), \n               TOTAL_WAGES = total_annual_wages) |&gt;\n        select(-area_fips, \n               -industry_code, \n               -annual_avg_emplvl, \n               -total_annual_wages) |&gt;\n        # 10 is a special value: \"all industries\" , so omit\n        filter(INDUSTRY != 10) |&gt; \n        mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n    })) |&gt; bind_rows()\n    \n    write_csv(ALL_DATA, fname)\n  }\n  \n  ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n  \n  ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n  \n  YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n  \n  if(length(YEARS_DIFF) &gt; 0){\n    stop(\"Download failed for the following years: \", YEARS_DIFF, \n         \". Please delete intermediate files and try again.\")\n  }\n  \n  ALL_DATA\n}\n\nWAGES &lt;- get_bls_qcew_annual_averages()\n\nTask 2 Question 1\n\nlibrary(dplyr)\nlibrary(readr)\n\nPERMITS &lt;- PERMITS %&gt;%\n  mutate(CBSA = as.character(CBSA))\n\nINCOME &lt;- INCOME %&gt;%\n  mutate(GEOID = as.character(GEOID))\n\nINCOME_unique &lt;- INCOME %&gt;%\n  group_by(GEOID) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  select(GEOID, NAME)\n\ntop_cbsa &lt;- PERMITS %&gt;%\n  filter(year &gt;= 2010 & year &lt;= 2019) %&gt;%\n  group_by(CBSA) %&gt;%\n  summarize(total_units = sum(new_housing_units_permitted, na.rm = TRUE)) %&gt;%\n  arrange(desc(total_units)) %&gt;%\n  slice(1)\n\ntop_cbsa_named &lt;- top_cbsa %&gt;%\n  left_join(INCOME_unique, by = c(\"CBSA\" = \"GEOID\"))\n\ncat(\"The CBSA that permitted the largest number of new housing units from 2010 to 2019 is\",\n  top_cbsa_named$NAME, \"with a total of\",\n  format(top_cbsa_named$total_units, big.mark = \",\"), \"units.\\n\")\n\nThe CBSA that permitted the largest number of new housing units from 2010 to 2019 is Houston-Sugar Land-Baytown, TX Metro Area with a total of 482,075 units.\n\n\nQuestion 2\n\nlibrary(dplyr)\n\nPERMITS &lt;- PERMITS %&gt;% mutate(CBSA = as.character(CBSA))\n\nalbuquerque_permits &lt;- PERMITS %&gt;%\n  filter(CBSA == \"10740\")\n\nalbuquerque_max &lt;- albuquerque_permits %&gt;%\n  arrange(desc(new_housing_units_permitted)) %&gt;%\n  slice(1) %&gt;%\n  select(year, new_housing_units_permitted)\n\ncat(\"Albuquerque, NM (CBSA 10740) permitted the most new housing units in\",\n  albuquerque_max$year, \"with a total of\",\n  format(albuquerque_max$new_housing_units_permitted, big.mark = \",\"), \"units.\\n\")\n\nAlbuquerque, NM (CBSA 10740) permitted the most new housing units in 2021 with a total of 4,021 units.\n\n\nQuestion 3\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidycensus)\n\nINCOME &lt;- get_acs(\n  geography = \"cbsa\",\n  variables = \"B19013_001\",\n  year = 2015,\n  survey = \"acs1\") %&gt;%\n  rename(household_income = estimate)\n\nGetting data from the 2015 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nWarning: • You have not set a Census API key. Users without a key are limited to 500\nqueries per day and may experience performance limitations.\nℹ For best results, get a Census API key at\nhttp://api.census.gov/data/key_signup.html and then supply the key to the\n`census_api_key()` function to use it throughout your tidycensus session.\nThis warning is displayed once per session.\n\nHOUSEHOLDS &lt;- get_acs(\n  geography = \"cbsa\",\n  variables = \"B11001_001\",\n  year = 2015,\n  survey = \"acs1\") %&gt;%\n  rename(households = estimate)\n\nGetting data from the 2015 1-year ACS\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\nPOPULATION_clean1 &lt;- get_acs(\n  geography = \"cbsa\",\n  variables = \"B01003_001\",\n  year = 2015,\n  survey = \"acs1\") %&gt;%\n  rename(population = estimate)\n\nGetting data from the 2015 1-year ACS\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\nCBSA_2015 &lt;- INCOME %&gt;%\n  left_join(HOUSEHOLDS, by = \"GEOID\") %&gt;%\n  left_join(POPULATION_clean1, by = \"GEOID\")\n\nCBSA_2015 &lt;- transform(\n  CBSA_2015,\n  total_income = household_income * households,\n  state = str_replace(str_extract(NAME, \", (.{2})\"), \", \", \"\"))\n\nSTATE_INCOME &lt;- CBSA_2015 %&gt;%\n  group_by(state) %&gt;%\n  summarize(\n    total_income_state = sum(total_income, na.rm = TRUE),\n    total_population_state = sum(population, na.rm = TRUE),\n    avg_individual_income = total_income_state / total_population_state,\n    .groups = \"drop\")\n\nstate_df &lt;- data.frame(\n  abb  = c(state.abb, \"DC\", \"PR\"),\n  name = c(state.name, \"District of Columbia\", \"Puerto Rico\"))\n\nSTATE_INCOME &lt;- STATE_INCOME %&gt;%\n  left_join(state_df, by = c(\"state\" = \"abb\"))\n\ntop_state &lt;- STATE_INCOME %&gt;%\n  arrange(desc(avg_individual_income)) %&gt;%\n  slice(1)\n\ncat(\"The state with the highest average individual income in 2015 is\",\n  top_state$name, \"with an average income of $\",\n  format(round(top_state$avg_individual_income, 2), big.mark = \",\"), \".\\n\")\n\nThe state with the highest average individual income in 2015 is District of Columbia with an average income of $ 33,232.88 .\n\n\nQuestion 4\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\n\ndata_scientists &lt;- WAGES %&gt;%\n  filter(INDUSTRY == 5182) %&gt;%\n  group_by(FIPS, YEAR) %&gt;%\n  summarize(total_employment = sum(EMPLOYMENT, na.rm = TRUE), .groups = \"drop\")\n\ntop_cbsa_each_year &lt;- data_scientists %&gt;%\n  group_by(YEAR) %&gt;%\n  slice_max(order_by = total_employment, n = 1, with_ties = FALSE) %&gt;%\n  ungroup()\n\ncbsa_names &lt;- INCOME %&gt;%\n  select(GEOID, NAME)\ncbsa_names$std_cbsa &lt;- paste0(\"C\", cbsa_names$GEOID)\n\ntop_cbsa_each_year$std_cbsa &lt;- paste0(top_cbsa_each_year$FIPS, \"0\")\n\ntop_cbsa_each_year &lt;- left_join(top_cbsa_each_year, cbsa_names, by = \"std_cbsa\")\n\ndatatable(\n  top_cbsa_each_year %&gt;%\n    arrange(YEAR) %&gt;%\n    select(YEAR, NAME, total_employment),\n  caption = \"Top CBSA for Data Scientists by Employment (5182)\",\n  options = list(pageLength = 10))\n\n\n\n\n\nQuestion 5\n\nlibrary(dplyr)\nlibrary(stringr)\n\nnyc_cbsa &lt;- WAGES %&gt;%\n  filter(str_detect(FIPS, \"C1018|C1038|C1042|C1050|C1058\")) %&gt;%\n  pull(FIPS) %&gt;%\n  unique()\n\nnyc_finance &lt;- WAGES %&gt;%\n  filter(FIPS %in% nyc_cbsa) %&gt;%\n  group_by(YEAR) %&gt;%\n  summarise(\n    total_wages = sum(TOTAL_WAGES, na.rm = TRUE),\n    fin_wages = sum(TOTAL_WAGES[str_starts(as.character(INDUSTRY), \"52\")], na.rm = TRUE),\n    .groups = \"drop\")\n\nnyc_finance$share_fin &lt;- nyc_finance$fin_wages / nyc_finance$total_wages\n\npeak &lt;- nyc_finance %&gt;%\n  filter(share_fin == max(share_fin, na.rm = TRUE))\n\ncat(\"In\", peak$YEAR, \",\",\n  round(peak$share_fin * 100, 2),\"% of NYC total wages were in Finance & Insurance (NAICS 52), the peak in the period.\\n\")\n\nIn 2021 , 5.08 % of NYC total wages were in Finance & Insurance (NAICS 52), the peak in the period.\n\n\nTask 3\nQuestion 1\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nINCOME &lt;- read_csv(\"data/mp02/B19013_001_cbsa_2009_2023.csv\", show_col_types = FALSE)\n\nINCOME &lt;- INCOME %&gt;%\n  rename(household_income = B19013_001)\n\nINCOME &lt;- INCOME %&gt;% mutate(year = as.numeric(.data$year))\nRENT   &lt;- RENT   %&gt;% mutate(year = as.numeric(.data$year))\n\nincome_rent &lt;- inner_join(INCOME, RENT, by = c(\"GEOID\", \"year\"))\n\ndata_2009 &lt;- income_rent %&gt;% filter(.data$year == 2009)\n\nggplot(data_2009, aes(x = household_income, y = monthly_rent)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Monthly Rent vs. Average Household Income per CBSA (2009)\",\n    x = \"Average Household Income (USD)\",\n    y = \"Average Monthly Rent (USD)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nQuestion 2\n\nlibrary(dplyr)\nlibrary(tidycensus)\nlibrary(ggplot2)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(purrr)\n\nyears &lt;- 2013:2019\n\nget_employment_data &lt;- function(y) {\n  total_emp &lt;- get_acs(\n    geography = \"cbsa\",\n    variables = \"B23025_003\",\n    year = y,\n    survey = \"acs1\"\n  ) %&gt;% rename(total_employment = estimate)\n  \n  healthcare_emp &lt;- get_acs(\n    geography = \"cbsa\",\n    variables = \"C24010_005\",\n    year = y,\n    survey = \"acs1\"\n  ) %&gt;% rename(healthcare_employment = estimate)\n  \n  total_emp %&gt;%\n    left_join(healthcare_emp, by = \"GEOID\") %&gt;%\n    mutate(NAME = total_emp$NAME, YEAR = y) %&gt;%\n    select(GEOID, NAME, YEAR, total_employment, healthcare_employment)\n}\n\nemployment_data &lt;- map_dfr(years, get_employment_data)\n\nGetting data from the 2013 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2013 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2014 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2014 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2015 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2015 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2016 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2016 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2017 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2017 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2018 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2018 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2019 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\nGetting data from the 2019 1-year ACS\n\n\nThe 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\nselected_cbsa &lt;- c(\"35620\", \"31080\", \"16980\", \"14460\")\nemployment_data_4 &lt;- employment_data %&gt;%\n  filter(GEOID %in% selected_cbsa)\n\nggplot(employment_data_4, aes(\n  x = total_employment,\n  y = healthcare_employment\n)) +\n  geom_point(aes(color = YEAR), size = 3) +\n  geom_line(aes(group = 1), color = \"gray\", alpha = 0.7) +\n  facet_wrap(~ NAME, scales = \"free\") +\n  scale_x_log10(labels = comma) + \n  scale_y_continuous(labels = comma) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Year\") +\n  labs(\n    title = \"Health Care & Social Services Employment vs Total Employment\",\n    subtitle = \"Selected CBSAs including NYC (2013–2019)\",\n    x = \"Total Employment (log scale)\",\n    y = \"Health Care & Social Services Employment (NAICS 62)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 11),\n    axis.title = element_text(size = 12))\n\n\n\n\n\n\n\n\nQuestion 3\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readr)\n\nPOPULATION &lt;- read_csv(\"data/mp02/B01003_001_cbsa_2009_2023.csv\", show_col_types = FALSE) %&gt;%\n  rename(population = B01003_001)\n\nHOUSEHOLDS &lt;- read_csv(\"data/mp02/B11001_001_cbsa_2009_2023.csv\", show_col_types = FALSE) %&gt;%\n  rename(households = B11001_001)\n\nCBSA_HH &lt;- POPULATION %&gt;%\n  select(GEOID, NAME, year, population) %&gt;%\n  left_join(\n    HOUSEHOLDS %&gt;% select(GEOID, year, households),\n    by = c(\"GEOID\", \"year\")) %&gt;%\n  mutate(avg_household_size = population / households)\n\ntop_cities &lt;- CBSA_HH %&gt;%\n  filter(year == max(year)) %&gt;%\n  arrange(desc(population)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  pull(NAME)\n\nCBSA_HH_filtered &lt;- CBSA_HH %&gt;%\n  filter(NAME %in% top_cities)\n\nggplot(CBSA_HH_filtered, aes(x = year, y = avg_household_size, color = NAME)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  scale_x_continuous(breaks = seq(min(CBSA_HH_filtered$year), max(CBSA_HH_filtered$year), by = 1)) +\n  labs(\n    title = \"Average Household Size Over Time\",\n    x = \"Year\",\n    y = \"Average Household Size\",\n    color = \"City\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 10))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nTask 4\nTable 1\n\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(DT)\n\nRENT_BURDEN &lt;- INCOME %&gt;%\n  inner_join(RENT, by = c(\"GEOID\", \"NAME\", \"year\"))\n\nRENT_BURDEN &lt;- RENT_BURDEN %&gt;%\n  mutate(rent_to_income_percentage = ((monthly_rent * 12) / household_income) * 100)\n\nNATIONAL_AVG &lt;- RENT_BURDEN %&gt;%\n  group_by(year) %&gt;%\n  summarize(national_avg_rti = mean(rent_to_income_percentage, na.rm = TRUE))\n\nbaseline_rti &lt;- NATIONAL_AVG %&gt;%\n  filter(year == 2009) %&gt;%\n  pull(national_avg_rti)\n\nRENT_BURDEN &lt;- RENT_BURDEN %&gt;%\n  mutate(rent_burden_index = (rent_to_income_percentage / baseline_rti) * 100)\n\nnyc_name &lt;- RENT_BURDEN %&gt;%\n  filter(str_detect(NAME, \"New York\")) %&gt;%\n  distinct(NAME) %&gt;%\n  pull(NAME) %&gt;%\n  first()\n\nprint(glue(\"Using metro name: {nyc_name}\"))\n\nUsing metro name: New York-Northern New Jersey-Long Island, NY-NJ-PA Metro Area\n\nnyc_rent &lt;- RENT_BURDEN %&gt;%\n  filter(NAME == nyc_name) %&gt;%\n  select(year, household_income, monthly_rent, rent_to_income_percentage, rent_burden_index) %&gt;%\n  mutate(rent_to_income_percentage = round(rent_to_income_percentage, 2),\n         rent_burden_index = round(rent_burden_index, 2))\n\ndatatable(\n  nyc_rent,\n  caption = glue(\"Rent Burden Over Time — {nyc_name}\"),\n  options = list(pageLength = 10))\n\n\n\n\n\nTable 2\n\nlatest_year &lt;- max(RENT_BURDEN$year, na.rm = TRUE)\n\ntop_bottom &lt;- RENT_BURDEN %&gt;%\n  filter(year == latest_year) %&gt;%\n  group_by(NAME) %&gt;%\n  summarize(avg_index = mean(rent_burden_index, na.rm = TRUE)) %&gt;%\n  arrange(desc(avg_index)) %&gt;%\n  mutate(rank = row_number())\n\ndatatable(\n  bind_rows(\n    head(top_bottom, 10),\n    tail(top_bottom, 10)\n  ),\n  caption = glue(\"Top & Bottom 10 Metro Areas by Rent Burden Index ({latest_year})\"),\n  options = list(pageLength = 20))\n\n\n\n\n\nTask 5\n\nlibrary(dplyr)\nlibrary(RcppRoll)\nlibrary(DT)\n\nPOPULATION_clean1 &lt;- POPULATION %&gt;%\n  rename(CBSA = GEOID) %&gt;%\n  mutate(CBSA = as.character(CBSA))\n\nPERMITS_clean1 &lt;- PERMITS %&gt;%\n  rename(CBSA = CBSA,\n         permits = new_housing_units_permitted) %&gt;%\n  mutate(CBSA = as.character(CBSA)) \n\nhousing_data &lt;- POPULATION_clean1 %&gt;%\n  left_join(PERMITS_clean1, by = c(\"CBSA\", \"year\"))\n\nhousing_data &lt;- housing_data %&gt;%\n  group_by(CBSA) %&gt;%\n  arrange(year, .by_group = TRUE) %&gt;%\n  mutate(\n    pop_5yrs_ago = lag(population, 5),\n    pop_growth_5yr = population - pop_5yrs_ago) %&gt;%\n  filter(year &gt;= 2014) %&gt;%  \n  ungroup()\n\nhousing_data &lt;- housing_data %&gt;%\n  mutate(\n    housing_growth_instant = (permits / population) * 1000,\n    housing_growth_rate = if_else(pop_growth_5yr &gt; 0, permits / pop_growth_5yr, 0))\n\nhousing_data &lt;- housing_data %&gt;%\n  mutate(\n    instant_z = scale(housing_growth_instant)[,1],\n    rate_z = scale(housing_growth_rate)[,1],\n    composite_score = instant_z + rate_z)\n\nhousing_data &lt;- housing_data %&gt;%\n  group_by(CBSA) %&gt;%\n  arrange(year, .by_group = TRUE) %&gt;%\n  mutate(\n    instant_roll5 = roll_mean(housing_growth_instant, 5, fill = NA, align = \"right\"),\n    rate_roll5 = roll_mean(housing_growth_rate, 5, fill = NA, align = \"right\")\n  ) %&gt;%\n  ungroup()\n\ntop_bottom_instant &lt;- housing_data %&gt;%\n  filter(!is.na(instant_z)) %&gt;%\n  arrange(desc(instant_z)) %&gt;%\n  slice(c(1:10, (n()-9):n())) %&gt;%\n  select(CBSA, NAME, year, population, permits, housing_growth_instant, instant_z)\n\ntop_bottom_rate &lt;- housing_data %&gt;%\n  filter(!is.na(rate_z)) %&gt;%\n  arrange(desc(rate_z)) %&gt;%\n  slice(c(1:10, (n()-9):n())) %&gt;%\n  select(CBSA, NAME, year, population, permits, housing_growth_rate, rate_z)\n\ntop_bottom_composite &lt;- housing_data %&gt;%\n  filter(!is.na(composite_score)) %&gt;%\n  arrange(desc(composite_score)) %&gt;%\n  slice(c(1:10, (n()-9):n())) %&gt;%\n  select(CBSA, NAME, year, population, permits, composite_score)\n\ndatatable(top_bottom_instant, \n          caption = \"Top and Bottom CBSAs — Instant Housing Growth (Z-Score)\",\n          options = list(pageLength = 10))\n\n\n\n\ndatatable(top_bottom_rate, \n          caption = \"Top and Bottom CBSAs — Housing Growth Rate (Z-Score)\",\n          options = list(pageLength = 10))\n\n\n\n\ndatatable(top_bottom_composite, \n          caption = \"Top and Bottom CBSAs — Composite Growth Score\",\n          options = list(pageLength = 10))\n\n\n\n\n\nTask 6\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(RcppRoll)\n\nPOPULATION_clean1 &lt;- POPULATION %&gt;%\n  rename(CBSA = GEOID) %&gt;%\n  select(CBSA, NAME, year, population) %&gt;%\n  mutate(CBSA = as.character(CBSA))\n\nRENT_clean &lt;- RENT %&gt;%\n  rename(CBSA = GEOID) %&gt;%\n  select(CBSA, NAME, year, monthly_rent) %&gt;%\n  mutate(CBSA = as.character(CBSA))\n\nPERMITS_clean &lt;- PERMITS %&gt;%\n  select(CBSA, new_housing_units_permitted, year) %&gt;%\n  mutate(CBSA = as.character(CBSA))\n\nhousing_data &lt;- POPULATION_clean1 %&gt;%\n  left_join(PERMITS_clean, by = c(\"CBSA\", \"year\")) %&gt;%\n  group_by(CBSA) %&gt;%\n  arrange(year, .by_group = TRUE) %&gt;%\n  mutate(\n    pop_5yrs_ago = lag(population, 5),\n    pop_growth_5yr = population - pop_5yrs_ago,\n    housing_growth_instant = (new_housing_units_permitted / population) * 1000,\n    housing_growth_rate = if_else(pop_growth_5yr &gt; 0,new_housing_units_permitted / pop_growth_5yr, 0)) %&gt;%\n  ungroup()\n\nexclude_cols &lt;- c(\"GEOID\", \"NAME\", \"year\", \"moe\", \"variable\")\nincome_candidates &lt;- setdiff(names(INCOME), exclude_cols)\nnum_candidates &lt;- income_candidates[sapply(INCOME[income_candidates], is.numeric)]\n\nincome_col &lt;- num_candidates[1]\nmessage(\"Detected income column: \", income_col)\n\nDetected income column: household_income\n\nrent_burden &lt;- RENT_clean %&gt;%\n  left_join(\n    INCOME %&gt;%\n      mutate(GEOID = as.character(GEOID)) %&gt;%\n      select(GEOID, year, !!sym(income_col)) %&gt;%\n      rename(household_income = !!sym(income_col)),\n    by = c(\"CBSA\" = \"GEOID\", \"year\")) %&gt;%\n  mutate(\n    rent_burden_ratio = (monthly_rent * 12) / household_income) %&gt;%\n  select(CBSA, NAME, year, rent_burden_ratio)\n\n\nmerged_data &lt;- housing_data %&gt;%\n  mutate(CBSA = as.character(CBSA)) %&gt;%\n  left_join(\n    rent_burden %&gt;% mutate(CBSA = as.character(CBSA)),\n    by = c(\"CBSA\", \"NAME\", \"year\"))\n\nglimpse(merged_data)\n\nRows: 7,279\nColumns: 10\n$ CBSA                        &lt;chr&gt; \"10140\", \"10140\", \"10140\", \"10140\", \"10140…\n$ NAME                        &lt;chr&gt; \"Aberdeen, WA Micro Area\", \"Aberdeen, WA M…\n$ year                        &lt;dbl&gt; 2009, 2010, 2011, 2012, 2013, 2014, 2015, …\n$ population                  &lt;dbl&gt; 71797, 72882, 72546, 71692, 71078, 70818, …\n$ new_housing_units_permitted &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ pop_5yrs_ago                &lt;dbl&gt; NA, NA, NA, NA, NA, 71797, 72882, 72546, 7…\n$ pop_growth_5yr              &lt;dbl&gt; NA, NA, NA, NA, NA, -979, -1760, -918, 100…\n$ housing_growth_instant      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ housing_growth_rate         &lt;dbl&gt; NA, NA, NA, NA, NA, 0.00000000, 0.00000000…\n$ rent_burden_ratio           &lt;dbl&gt; 0.2146100, 0.2078013, 0.1970092, 0.1848919…\n\nsummary(merged_data$rent_burden_ratio)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1231  0.1738  0.1921  0.1966  0.2156  0.3819 \n\nlibrary(dplyr)\n\ncbsa_summary &lt;- merged_data %&gt;%\n  group_by(CBSA, NAME) %&gt;%\n  summarize(\n    rent_burden_early = mean(rent_burden_ratio[year %in% 2009:2012], na.rm = TRUE),\n    rent_burden_recent = mean(rent_burden_ratio[year %in% 2019:2023], na.rm = TRUE),\n    rent_burden_change = rent_burden_recent - rent_burden_early,\n    pop_early = mean(population[year %in% 2009:2012], na.rm = TRUE),\n    pop_recent = mean(population[year %in% 2019:2023], na.rm = TRUE),\n    pop_growth_rate = (pop_recent - pop_early) / pop_early,\n    housing_growth_avg = mean(housing_growth_instant, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'CBSA'. You can override using the\n`.groups` argument.\n\n#Graph 1#\nlibrary(ggplot2)\n\nggplot(cbsa_summary, aes(x = housing_growth_avg, y = -rent_burden_change)) +\n  geom_point(alpha = 0.6, color = \"blue\") +\n  geom_vline(xintercept = mean(cbsa_summary$housing_growth_avg, na.rm = TRUE), linetype = \"dashed\", color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"YIMBY Scorecard: Housing Growth vs Rent Burden Change (2009–2023)\",\n    subtitle = \"Top-right quadrant: High housing growth + falling rent burden\",\n    x = \"Average Housing Growth (permits per 1,000 residents)\",\n    y = \"Decrease in Rent Burden (negative change = improvement)\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 470 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nTable 2\n\nggplot(cbsa_summary, aes(x = pop_growth_rate * 100, y = -rent_burden_change)) +\n  geom_point(aes(size = housing_growth_avg, color = housing_growth_avg), alpha = 0.6) +\n  scale_color_viridis_c(option = \"plasma\") +\n  labs(\n    title = \"Population Growth vs Rent Burden Change (2009–2023)\",\n    subtitle = \"Bubble size and color represent housing growth intensity\",\n    x = \"Population Growth Rate (%)\",\n    y = \"Decrease in Rent Burden (negative change = improvement)\",\n    color = \"Housing Growth\\n(per 1,000 residents)\"\n  ) +\n  theme_minimal()\n\nWarning: Removed 470 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nTask 7\nYIMBY Policy Bill Policy Brief\nYIMBY refers to “Yes In My Backyard”, which advocates for new development, covering construction and real estate from a pro-growth perspective. Given the cost of living crisis we have seen in recent years, where rent prices have far surpassed the average American’s salary, we must implement policies to increase housing supply and ease this cost of living crisis.\nTo identify high YIMBY cities, the government can track two key metrics: rent burden and housing growth. The rent burden metric measures the share of a household’s income spent on rent, reflecting overall housing affordability. Meanwhile, the housing growth metric measures the rate at which new housing units are being built, indicating whether cities are expanding their supply to meet demand. This bill intends to support cities that are at least working to increase housing supply and create incentives for this to continue, so that the rent burden becomes lower.\nTo address this crisis, we have identified Representative Ilhan Omar (MN-05) as the sponsor of our YIMBY bill. Her district is in Minneapolis, which is a success story in the YIMBY movement. In 2019, it became the first major U.S. city to eliminate single-family-only zoning, a move that spurred the development of duplexes and other middle-class housing (Smith, 2022). Given this background, as well as her record for sponsoring public housing bills, including legislation calling for 9.5 million new public housing units, we believe that she is the best sponsor for our bill (Ibid).\nFor co-sponsor, we believe that Robert Garcia (CA-42) is the prime candidate for the co-sponsor of our YIMBY bill. Despite opposition from residents in his district of Long Beach, Robert Garcia is one of the biggest proponents of YIMBY in Congress. Given both their support, we believe that these 2 congressmen are the best duo to sponsor our bills.\nThis YIMBY bill would provide significant benefits to both teachers and construction workers in Minnesota and Long Beach, California, as these two occupations represent large portions of the local workforce and are backed by influential unions. The teachers are backed by the Teachers Association of Long Beach and Education Minnesota. In both regions, teachers face growing challenges with housing affordability. In Long Beach, the median rent for a one-bedroom apartment exceeds $2,000 per month, while the average teacher salary is around $72,000, making it difficult for educators to live near their schools (Long Beach Unified School District). Similarly, in Minnesota, teachers also struggle with rising housing costs. By increasing the housing supply through zoning reform and promoting multifamily development near schools and public transit, this bill would help lower rent burdens.\nEven a modest rent reduction would allow teachers to save thousands of dollars per year, improving their financial stability and job satisfaction. Lower housing costs would also make it easier for school districts to attract and retain qualified teachers, directly supporting educational outcomes. This bill would also have a strong positive impact on construction workers, who are represented by powerful labor unions such as the Minnesota Building and Construction Trades Council and the Southern California District Council of Laborers. Encouraging more housing construction provides steady, union-backed employment and wage growth.\nOverall, the YIMBY bill supports two key constituencies essential to both Minnesota and Long Beach. By lowering rent burdens for teachers and creating thousands of new jobs for unionized construction labor, the bill delivers broad economic and social benefits and makes it an attractive and practical policy for representatives.\nBibliography\n“Salary & Benefits.” Long Beach Unified School District, www.lbschools.net/departments/human-resource-services/for-current-employees/salary-benefits. Accessed 30 Oct. 2025.\nSmith, Carl. “Ending Single-Family Zoning Is Not a Stand-Alone Solution.” Governing, Governing, 21 Jan. 2022, www.governing.com/community/ending-single-family-zoning-is-not-a-stand-alone-solution."
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "MiniProject 01 - Netflix Data Analysis",
    "section": "",
    "text": "We are currently working to analyze how films and TV series have performed on Netflix in the past four years. By taking a look at this data, we will see how Netflix is performing in both US market and the international market, which is important for its growth strategy. To conduct our analysis for Netflix, we are going to download 2 key datasets. The country dataset shows weekly Netflix rankings by country. The global dataset tracks global content performance with viewership and ranking metrics.\n\nif(!dir.exists(file.path(\"data\", \"mp01\"))){\n  dir.create(file.path(\"data\", \"mp01\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\n\nif(!file.exists(GLOBAL_TOP_10_FILENAME)){\n  download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\", \n                destfile=GLOBAL_TOP_10_FILENAME)\n}\n\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif(!file.exists(COUNTRY_TOP_10_FILENAME)){\n  download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\", \n                destfile=COUNTRY_TOP_10_FILENAME)\n}\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readr)\nlibrary(dplyr)\n\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\n\nRows: 8840 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (3): category, show_title, season_title\ndbl  (5): weekly_rank, weekly_hours_viewed, runtime, weekly_views, cumulativ...\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(GLOBAL_TOP_10)\n\nspc_tbl_ [8,840 × 9] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ week                      : Date[1:8840], format: \"2025-09-21\" \"2025-09-21\" ...\n $ category                  : chr [1:8840] \"Films (English)\" \"Films (English)\" \"Films (English)\" \"Films (English)\" ...\n $ weekly_rank               : num [1:8840] 1 2 3 4 5 6 7 8 9 10 ...\n $ show_title                : chr [1:8840] \"The Wrong Paris\" \"KPop Demon Hunters\" \"Ice Road: Vengeance\" \"aka Charlie Sheen\" ...\n $ season_title              : chr [1:8840] \"N/A\" \"N/A\" \"N/A\" \"aka Charlie Sheen: Season 1\" ...\n $ weekly_hours_viewed       : num [1:8840] 38900000 35400000 14400000 21800000 10900000 7100000 7800000 6300000 5800000 4000000 ...\n $ runtime                   : num [1:8840] 1.78 1.67 1.88 3.03 1.7 ...\n $ weekly_views              : num [1:8840] 21800000 21200000 7600000 7200000 6400000 4500000 3900000 3400000 3100000 2800000 ...\n $ cumulative_weeks_in_top_10: num [1:8840] 2 14 1 2 2 4 4 1 1 1 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   week = col_date(format = \"\"),\n  ..   category = col_character(),\n  ..   weekly_rank = col_double(),\n  ..   show_title = col_character(),\n  ..   season_title = col_character(),\n  ..   weekly_hours_viewed = col_double(),\n  ..   runtime = col_double(),\n  ..   weekly_views = col_double(),\n  ..   cumulative_weeks_in_top_10 = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nglimpse(GLOBAL_TOP_10)\n\nRows: 8,840\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; \"N/A\", \"N/A\", \"N/A\", \"aka Charlie Sheen: Se…\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n\n\nTask 1\n\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt;\n  mutate(season_title = if_else(season_title == \"N/A\", NA_character_, season_title))\nglimpse(GLOBAL_TOP_10)\n\nRows: 8,840\nColumns: 9\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films (English)\", \"Films (English)\", \"Film…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Wrong Paris\", \"KPop Demon Hunters\", \"I…\n$ season_title               &lt;chr&gt; NA, NA, NA, \"aka Charlie Sheen: Season 1\", …\n$ weekly_hours_viewed        &lt;dbl&gt; 38900000, 35400000, 14400000, 21800000, 109…\n$ runtime                    &lt;dbl&gt; 1.7833, 1.6667, 1.8833, 3.0333, 1.7000, 1.5…\n$ weekly_views               &lt;dbl&gt; 21800000, 21200000, 7600000, 7200000, 64000…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 2, 14, 1, 2, 2, 4, 4, 1, 1, 1, 1, 2, 5, 1, …\n\n\nTask 2\n\nif(!require(\"readr\")) install.packages(\"readr\")\nlibrary(readr)\n\nCOUNTRY_TOP_10 &lt;- read_tsv(\n  COUNTRY_TOP_10_FILENAME,\n  na = \"N/A\"\n)\n\nRows: 411760 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (5): country_name, country_iso2, category, show_title, season_title\ndbl  (2): weekly_rank, cumulative_weeks_in_top_10\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(COUNTRY_TOP_10)\n\nRows: 411,760\nColumns: 8\n$ country_name               &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Arg…\n$ country_iso2               &lt;chr&gt; \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"AR\", \"…\n$ week                       &lt;date&gt; 2025-09-21, 2025-09-21, 2025-09-21, 2025-0…\n$ category                   &lt;chr&gt; \"Films\", \"Films\", \"Films\", \"Films\", \"Films\"…\n$ weekly_rank                &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, …\n$ show_title                 &lt;chr&gt; \"The Mule\", \"The Wrong Paris\", \"KPop Demon …\n$ season_title               &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Ma…\n$ cumulative_weeks_in_top_10 &lt;dbl&gt; 1, 2, 14, 1, 1, 1, 2, 1, 5, 1, 2, 1, 7, 1, …\n\n\nTask 3\n\nlibrary(DT)\nGLOBAL_TOP_10 |&gt; \n  head(n=20) |&gt;\n  datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\nlibrary(stringr)\nformat_titles &lt;- function(df){\n  colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n  df\n}\n\nGLOBAL_TOP_10 |&gt; \n  format_titles() |&gt;\n  head(n=20) |&gt;\n  datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n  formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\nGLOBAL_TOP_10 |&gt; \n  select(-season_title) |&gt;\n  format_titles() |&gt;\n  head(n=20) |&gt;\n  datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n  formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\nGLOBAL_TOP_10 |&gt; \n  mutate(`Runtime (Minutes)` = round(60 * runtime)) |&gt;  # use proper column name here\n  select(-season_title, -runtime) |&gt;\n  format_titles() |&gt;\n  head(n=20) |&gt;\n  datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n  formatRound(c('Weekly Hours Viewed', 'Weekly Views', 'Runtime (Minutes)'))\n\n\n\n\n\nFrom the data above, we were able to see that in the week of September 21, 2025, the top 20 most viewed shows on Netflix were all films, highlighting how films are still popular with their consumers.\nTask 4\nQuestion 1\n\nlibrary(dplyr)\n\nnum_countries &lt;- COUNTRY_TOP_10 |&gt; \n  distinct(country_name) |&gt; \n  count() |&gt; \n  pull(n)\n\ncat(\"Netflix operates in\", num_countries, \"different countries based on the viewing history data.\")\n\nNetflix operates in 94 different countries based on the viewing history data.\n\n\nQuestion 2\n\nlibrary(dplyr)\n\nnon_english_top &lt;- GLOBAL_TOP_10 |&gt; \n  filter(category == \"Films (Non-English)\") |&gt;\n  group_by(show_title) |&gt; \n  summarise(max_weeks = max(cumulative_weeks_in_top_10, na.rm = TRUE),.groups = \"drop\") |&gt; \n  arrange(desc(max_weeks)) |&gt; \n  slice(1)\n\ncat(\"The non-English-language film that spent the most cumulative weeks in the global top 10 is\",\n    non_english_top$show_title, \"with\",\n    non_english_top$max_weeks, \"weeks.\")\n\nThe non-English-language film that spent the most cumulative weeks in the global top 10 is All Quiet on the Western Front with 23 weeks.\n\n\nQuestion 3\n\nlongest_film &lt;- GLOBAL_TOP_10 |&gt; \n  filter(grepl(\"Films\", category)) |&gt;\n  filter(!is.na(runtime)) |&gt; \n  group_by(show_title) |&gt; \n  summarise(max_runtime = max(runtime, na.rm = TRUE), .groups = \"drop\") |&gt; \n  arrange(desc(max_runtime)) |&gt; \n  slice(1)\n\nruntime_minutes &lt;- round(longest_film$max_runtime * 60)\n\ncat(\"The longest film to have ever appeared in the Netflix global Top 10 is\",\n    longest_film$show_title, \"with a runtime of\",\n    runtime_minutes, \"minutes.\")\n\nThe longest film to have ever appeared in the Netflix global Top 10 is Pushpa 2: The Rule (Reloaded Version) with a runtime of 224 minutes.\n\n\nQuestion 4\n\nlibrary(DT)\nlibrary(dplyr)\nlibrary(stringr)\n\ntop_by_category &lt;- GLOBAL_TOP_10 %&gt;%\n  group_by(category, show_title) %&gt;%\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  group_by(category) %&gt;%\n  slice_max(order_by = total_hours, n = 1) %&gt;%\n  ungroup()\n\ntop_by_category %&gt;% datatable(options = list(searching = FALSE, info = FALSE), \ncaption = \"Most Viewed Program Globally by Category (Total Hours)\")\n\n\n\n\n\nQuestion 5\n\nlibrary(dplyr)\n\nLongest_Run_TV_show &lt;- COUNTRY_TOP_10 |&gt;\n  filter(!is.na(cumulative_weeks_in_top_10)) |&gt;\n  filter(grepl(\"TV\", category)) |&gt;\n  arrange(desc(cumulative_weeks_in_top_10)) |&gt;\n  slice(1)\n\ncat(\"The TV show that had the longest run in a country’s Top 10 is\",\n    Longest_Run_TV_show$show_title, \"with\",\n    Longest_Run_TV_show$cumulative_weeks_in_top_10, \"weeks in\",\n    Longest_Run_TV_show$country_name)\n\nThe TV show that had the longest run in a country’s Top 10 is Money Heist with 127 weeks in Pakistan\n\n\nQuestion 6\n\nlibrary(dplyr)\nlibrary(lubridate)\n\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 |&gt;\n  mutate(week = parse_date_time(week, orders = c(\"ymd\", \"mdy\", \"dmy\")))\n\ncountry_weeks &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(num_weeks = n_distinct(week),\n  last_week = max(week, na.rm = TRUE),\n  .groups = \"drop\") |&gt;\n  filter(num_weeks &lt; 200)\n\nif(nrow(country_weeks) &gt; 0){\n  cat(\"The country with fewer than 200 weeks of Netflix data is\", country_weeks$country_name,\n  \"and Netflix ceased operations there in\", year(country_weeks$last_week))} else {\n  cat(\"All countries have 200 or more weeks of Netflix data.\")}\n\nThe country with fewer than 200 weeks of Netflix data is Russia and Netflix ceased operations there in 2022\n\n\nQuestion 7\n\nlibrary(dplyr)\n\nsquid_game_total &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Squid Game\") |&gt;\n  summarise(\n    total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE)\n  )\n\ncat(\"The total global viewership of Squid Game across all seasons is\",\n    squid_game_total$total_hours_viewed, \"hours\")\n\nThe total global viewership of Squid Game across all seasons is 5048300000 hours\n\n\nQuestion 8\n\nlibrary(dplyr)\nlibrary(lubridate)\n\nred_notice_hours &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\",\n    year(week) == 2021) |&gt;\n  summarise(\n    total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE),\n    .groups = \"drop\")\n\nruntime_hours &lt;- 1 + 58/60\n\nred_notice_views &lt;- red_notice_hours$total_hours_viewed / runtime_hours\n\ncat(\"Red Notice had approximately\", round(red_notice_views, 0),\"views in 2021.\")\n\nRed Notice had approximately 201732203 views in 2021.\n\n\nQuestion 9\n\nlibrary(dplyr)\n\nus_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"United States\",\n  grepl(\"Films\", category))\n\nfilm_debuts &lt;- us_films |&gt;\n  group_by(show_title) |&gt;\n  filter(week == min(week)) |&gt;\n  summarise(\n    debut_week = min(week),\n    debut_rank = min(weekly_rank),\n    .groups = \"drop\")\n\nfilms_with_num1 &lt;- us_films |&gt;\n  filter(weekly_rank == 1) |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    first_num1_week = min(week),\n    .groups = \"drop\")\n\nclimbers &lt;- film_debuts |&gt;\n  inner_join(films_with_num1, by = \"show_title\") |&gt;\n  filter(debut_rank &gt; 1)\n\nmost_recent &lt;- climbers |&gt;\n  arrange(desc(first_num1_week)) |&gt;\n  slice(1)\n\ncat(\"Number of films that reached #1 after debuting lower:\",nrow(climbers),\"\\n\")\n\nNumber of films that reached #1 after debuting lower: 45 \n\ncat(\"The most recent film to do this was\",most_recent$show_title, \"\\n\")\n\nThe most recent film to do this was Unknown Number: The High School Catfish \n\n\nQuestion 10\n\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(stringr)\n\nCOUNTRY_TOP_10 &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(show_season = if_else(is.na(season_title),\n  show_title, paste(show_title, season_title, sep = \" - \")))\n\ntop_countries &lt;- COUNTRY_TOP_10 %&gt;%\n  group_by(show_season, country_name) %&gt;%\n  summarise(debut_week = min(week), .groups = \"drop\") %&gt;%\n  group_by(show_season) %&gt;%\n  summarise(countries_charted = n_distinct(country_name), .groups = \"drop\") %&gt;%\n  slice_max(order_by = countries_charted, n = 1) %&gt;%\n  ungroup() %&gt;%\n  as.data.frame()\n\ndatatable(\n  top_countries,\n  colnames = c(\"TV Show/Season\", \"Countries Charted\"),\n  caption = \"TV Show/Season That Hit Top 10 in the Most Countries During Debut Week\") %&gt;%\n  formatRound('countries_charted', 0)\n\n\n\n\n\nTask 5\nNetflix Announces the Fifth and Final Season of Stranger Things!\nToday, Netflix announces that the highly anticipated fifth and final season of Stranger Things will debut at the end of 2025. Ahead of this major news, the streaming giant highlighted the global impact of the series’s first four seasons, which redefined streaming and became one of Netflix’s greatest success stories.\n\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(stringr)\n\nstranger_global &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(show_title == \"Stranger Things\") %&gt;%\n  group_by(show_title, season_title) %&gt;%\n  summarise(\n    total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE),\n    total_weeks_in_top10 = n_distinct(week),\n    .groups = \"drop\")\n\nstranger_country &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(show_season = if_else(is.na(season_title), show_title,\n  paste(show_title, season_title, sep = \" - \"))) %&gt;%\n  filter(show_title == \"Stranger Things\") %&gt;%\n  group_by(show_season) %&gt;%\n  summarise(\n    countries_charted = n_distinct(country_name),\n    .groups = \"drop\")\n\nstranger_summary &lt;- stranger_global %&gt;%\n  mutate(show_season = if_else(is.na(season_title), show_title,\n  paste(show_title, season_title, sep = \" - \"))) %&gt;%\n  left_join(stranger_country, by = \"show_season\") %&gt;%\n  select(show_title, season_title, total_hours_viewed,\n  total_weeks_in_top10, countries_charted)\n\n\ndatatable(\n  stranger_summary,\n  colnames = c(\"Show Title\",\"Season\",\"Total Hours Viewed\",\"Weeks in Top 10\",\"Countries Charted\"),\n  caption = \"Stranger Things: Global Impact Across Seasons\") %&gt;%\n  formatRound('total_hours_viewed', 0)\n\n\n\n\n\nIn its first four seasons, Stranger Things has amassed over 2.9 billion total hours viewed, spending a combined 50 weeks in the Global Top 10, and charting in over 90 countries worldwide. These metrics show that the show can resonate across a wide range of audiences worldwide. Each season has seen greater success, resulting in the fourth season of Stranger Things generating an astounding 1.89 billion hours viewed and holding a place in the Global Top 10 for 19 weeks across 93 countries.\n\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(stringr)\n\nblackmirror_global &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(show_title == \"Black Mirror\") %&gt;%\n  group_by(show_title, season_title) %&gt;%\n  summarise(\n    total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE),\n    total_weeks_in_top10 = n_distinct(week),\n    .groups = \"drop\")\n\nblackmirror_country &lt;- COUNTRY_TOP_10 %&gt;%\n  mutate(show_season = if_else(is.na(season_title), show_title,\n  paste(show_title, season_title, sep = \" - \"))) %&gt;%\n  filter(show_title == \"Black Mirror\") %&gt;%\n  group_by(show_season) %&gt;%\n  summarise(\n    countries_charted = n_distinct(country_name),\n    .groups = \"drop\")\n\nblackmirror_summary &lt;- blackmirror_global %&gt;%\n  mutate(show_season = if_else(is.na(season_title), show_title,\n  paste(show_title, season_title, sep = \" - \"))) %&gt;%\n  left_join(blackmirror_country, by = \"show_season\") %&gt;%\n  select(show_title, season_title, total_hours_viewed,total_weeks_in_top10, countries_charted)\n\ndatatable(\n  blackmirror_summary,\n  colnames = c(\"Show Title\",\"Season\",\"Total Hours Viewed\",\"Weeks in Top 10\",\"Countries Charted\"),\n  caption = \"Black Mirror: Global Impact Across Seasons\") %&gt;%\n  formatRound('total_hours_viewed', 0)\n\n\n\n\n\nBy comparison, Black Mirror, another Netflix original series, accumulated 331.6 million hours viewed across 10 weeks in the Top 10, charting in over 90 countries.\nThese figures emphasize that Stranger Things is not only a national hit but also a global phenomenon. Its sustained popularity, record-breaking total viewership, and broad reach surpass many other English-language shows, solidifying its status as one of the most influential series of the streaming era. With the fifth and final season slated for release at the end of 2025, Stranger Things continues to define Netflix’s global success in the streaming business.\nTask 6\nNetflix Announces Massive Subscriber Growth in India!\nIn recent years, Netflix has seen massive growth in subscribers outside the United States. As growth in the United States has stagnated, Netflix looked to expand internationally and focused heavily on India, given that it is home to over 20% of the world’s population.\nNetflix spent hundreds of millions of dollars on both original content as well as acquiring Hindi shows. Several Indian titles achieved great runs on the platform as seen with The Great Indian Kapil Show, which has secured an impressive 44 weeks in the Top 10, cementing its position as a fan favorite.\nOther standout titles include:\n\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(stringr)\n\nindia_top &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"India\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(\n    weeks_in_top10_india = n_distinct(week),\n    .groups = \"drop\"\n  )\n\nus_top &lt;- COUNTRY_TOP_10 %&gt;%\n  filter(country_name == \"United States\") %&gt;%\n  group_by(show_title) %&gt;%\n  summarise(appeared_us = TRUE, .groups = \"drop\")\n\nindian_exclusive &lt;- india_top %&gt;%\n  left_join(us_top, by = \"show_title\") %&gt;%\n  filter(is.na(appeared_us)) %&gt;%\n  arrange(desc(weeks_in_top10_india)) %&gt;%\n  slice(-1)\n\ndatatable(\n  indian_exclusive,\n  colnames = c(\"Show Title\", \"Weeks in Top 10 (India)\"),\n  caption = \"Popular Shows in India That Did Not Chart in the US\"\n)\n\n\n\n\n\nThese rankings reflect how Indian shows resonate with local audiences. As Netflix India continues to invest in diverse, high-quality stories, fans can look forward to more titles that showcase the Indian culture and talent\nTask 7\nNetflix Data Reveals Increasing Popularity of Non-English Films\nNetflix’s latest viewership analysis highlights the continuing rise of international content on the platform. As seen below,so far in 2025, English-language films amassed 5,062,300,000 total hours viewed and spent over 38 weeks in the Top 10, covering 185 distinct titles. In comparison, though non-English films generated just over half the total hours viewed, it spent equal number of weeks in the Top 10 as English films.\n\nlibrary(dplyr)\nlibrary(DT)\n\nfilm_comparison_2025 &lt;- GLOBAL_TOP_10 %&gt;%\n  filter(year(week) == 2025, grepl(\"Films\", category)) %&gt;%\n  group_by(category) %&gt;%\n  summarise(\n    total_hours_viewed = sum(weekly_hours_viewed, na.rm = TRUE),\n    total_weeks_in_top10 = n_distinct(week),\n    number_of_titles = n_distinct(show_title),\n    .groups = \"drop\")\n\nfilm_comparison_2025 %&gt;%\n  datatable(\n    colnames = c(\"Language Type\", \"Total Hours Viewed\", \"Total Weeks in Top 10\", \"Number of Titles\"),\n    caption = \"Comparison of English vs Non-English Films on Netflix (2025)\") %&gt;%\n  formatRound(c(\"total_hours_viewed\", \"total_weeks_in_top10\", \"number_of_titles\"), 0)\n\n\n\n\n\nThis comparison shows that Netflix can deliver content across languages and regions. While English films continue to draw the largest cumulative audience, the robust viewership of non-English films shows the increasing global appetite for diverse and international content.\nInternational films have become a key part of the platform’s long-term growth strategy, and the platform’s international content library is helping expand engagement beyond traditional English-speaking markets, while also offering subscribers access to culturally relevant stories. These insights reinforce Netflix’s mission to provide a global entertainment experience that can successfully expand outside its traditional US market."
  }
]